{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting ./data/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting ./data/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./data/mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"./data/mnist\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data size: 55000.\n",
      "Dev data size: 5000.\n",
      "Testing data size: 10000.\n"
     ]
    }
   ],
   "source": [
    "print(\"Training data size: %d.\" % mnist.train.num_examples)\n",
    "print(\"Dev data size: %d.\" % mnist.validation.num_examples)\n",
    "print(\"Testing data size: %d.\" % mnist.test.num_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]\n",
      "(784,)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Label 0-9 (One-Hot Encoding)\n",
    "\"\"\"\n",
    "print(mnist.train.labels[0])\n",
    "\n",
    "\"\"\"\n",
    "Images 28x28 (flatten)\n",
    "\"\"\"\n",
    "print(mnist.train.images[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Start to train the neural network\n",
    "\"\"\"\n",
    "TRAINING_SIZE = mnist.train.num_examples\n",
    "\n",
    "INPUT_NODE = mnist.train.images[0].shape[0]\n",
    "OUTPUT_NODE = mnist.train.labels[0].shape[0]\n",
    "\n",
    "FIRST_LAYER = 500\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "START_LEARNING_RATE = 0.9\n",
    "LEARNING_RATE_DECAY = 0.99\n",
    "LAMBDA = 0.0001 # use to regularization\n",
    "\n",
    "NUM_OF_ITERATION = 50000\n",
    "\n",
    "MOVING_AVG_BETA = 0.99 # beta for moving avg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting ./data/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting ./data/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./data/mnist/t10k-labels-idx1-ubyte.gz\n",
      "After 10000 iterations, loss on training batch is 0.062118, validation accuracy is 0.983000.\n",
      "After 20000 iterations, loss on training batch is 0.039689, validation accuracy is 0.983600.\n",
      "After 30000 iterations, loss on training batch is 0.031362, validation accuracy is 0.984800.\n",
      "After 40000 iterations, loss on training batch is 0.030295, validation accuracy is 0.984400.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Forward Propagation\n",
    "\"\"\"\n",
    "\n",
    "def forward_prop(input_tensor, weights, bias, avg_class=None):\n",
    "    a1 = None\n",
    "    a2 = None\n",
    "    W1 = weights['W1']\n",
    "    W2 = weights['W2']\n",
    "    b1 = bias['b1']\n",
    "    b2 = bias['b2']\n",
    "    \n",
    "    if avg_class == None:\n",
    "        a1 = tf.nn.relu(tf.matmul(input_tensor, W1) + b1)\n",
    "        a2 = tf.matmul(a1, W2) + b2\n",
    "    else:\n",
    "        a1 = tf.nn.relu(tf.matmul(input_tensor, avg_class.average(W1)) + avg_class.average(b1))\n",
    "        a2 = tf.matmul(a1, avg_class.averate(W2)) + avg_class.average(b2)\n",
    " \n",
    "    return a2\n",
    "\n",
    "\"\"\"\n",
    "Training without moving average optimize\n",
    "\"\"\"\n",
    "def train(mnist):\n",
    "    iteration = tf.Variable(0, trainable=False)\n",
    "   \n",
    "    # tf.placeholder(type, shape, name)\n",
    "    x = tf.placeholder(tf.float32, [None, INPUT_NODE], name='x-input')\n",
    "    y_ = tf.placeholder(tf.float32, [None, OUTPUT_NODE], name='y-input')\n",
    "    \n",
    "    # initial with truncated normal distribution\n",
    "    W1 = tf.Variable(tf.truncated_normal([INPUT_NODE, FIRST_LAYER], stddev=0.1))\n",
    "    b1 = tf.Variable(tf.constant(0.1, shape=[FIRST_LAYER]))\n",
    "    W2 = tf.Variable(tf.truncated_normal([FIRST_LAYER, OUTPUT_NODE], stddev=0.1))\n",
    "    b2 = tf.Variable(tf.constant(0.1, shape=[OUTPUT_NODE]))\n",
    "    \n",
    "    weights = {\n",
    "        'W1' : W1,\n",
    "        'W2' : W2\n",
    "    }\n",
    "    \n",
    "    bias = {\n",
    "        'b1' : b1,\n",
    "        'b2' : b2\n",
    "    }\n",
    "    \n",
    "    # Forward-Prop    \n",
    "    y = forward_prop(x, weights, bias)\n",
    "   \n",
    "    # Loss fucntion (cross-entropy: use to compute categorical labels) \n",
    "    # sparse_softmax_cross_entropy_with_logits: compute cross entropy after applying softmax\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=y, labels=tf.argmax(y_, 1))\n",
    "    # Mean of batch loss\n",
    "    cross_entropy_mean = tf.reduce_mean(cross_entropy)\n",
    "    \n",
    "    # Reglurization\n",
    "    regularizer = tf.contrib.layers.l2_regularizer(LAMBDA)\n",
    "    regularization = regularizer(W1) + regularizer(W2)\n",
    "    \n",
    "    # Compute Loss\n",
    "    loss = cross_entropy_mean + regularization\n",
    "    \n",
    "    learning_rate = tf.train.exponential_decay(START_LEARNING_RATE, \n",
    "                                               iteration, \n",
    "                                               TRAINING_SIZE/BATCH_SIZE, \n",
    "                                               LEARNING_RATE_DECAY)\n",
    "    \n",
    "    # update weights\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    train_step = optimizer.minimize(loss, global_step=iteration)\n",
    "    \n",
    "    # compare result and prediction \n",
    "    correct_predict = tf.equal(tf.argmax(y_, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_predict, tf.float32))\n",
    "  \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        dev_feed = { \n",
    "            x: mnist.validation.images,\n",
    "            y_: mnist.validation.labels\n",
    "        }\n",
    "        \n",
    "        test_feed = {\n",
    "            x: mnist.test.images,\n",
    "            y_: mnist.test.labels\n",
    "        }\n",
    "        \n",
    "        for i in range(1, NUM_OF_ITERATION+1):\n",
    "            # training set\n",
    "            xs, ys = mnist.train.next_batch(BATCH_SIZE)\n",
    "            _, predict_values, loss_value, step = sess.run([train_step, y, loss, iteration], feed_dict={x: xs, y_: ys})\n",
    "            \n",
    "            if i % 10000 == 0:\n",
    "                # dev set\n",
    "                validate_acc = sess.run(accuracy, feed_dict=dev_feed)\n",
    "                \n",
    "                print(\"After %d iterations, loss on training batch is %f, \" \n",
    "                      \"validation accuracy is %f.\" % (step, loss_value, validate_acc))\n",
    "        \n",
    "        # testing set\n",
    "        test_acc = sess.run(accuracy, feed_dict=test_feed)\n",
    "        print(\"After training, testing accuracy is %f.\" % test_acc)\n",
    "        \n",
    "\n",
    "# main \n",
    "def run(argv=None):\n",
    "    mnist = input_data.read_data_sets('./data/mnist', one_hot=True)\n",
    "    train(mnist)\n",
    "\n",
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
